---
title: Customer Segmentation. Application of Unsupervised Learning Methods for Trend Exploration
author:
  - name: Ketao Li
    affiliation: York University
    email:  liketao@yahoo.com
  - name: Kush Halani
    affiliation: York University
    email:  kush.halani@ontariotechu.net
  - name: Josue Romain
    affiliation: York University
    email:  josue.rolland.romain@gmail.com    
  - name: Juan Peña
    affiliation: York University
    email:  jppena62@my.yorku.ca
 
abstract: >
  Customer segmentation is the process of dividing customers into groups based on common characteristics so companies can market to each group effectively and appropriately. 

output:
  rticles::rjournal_article:
    includes:
      in_header: preamble.tex
   
---

```{r echo=FALSE, message=FALSE, warnings=FALSE}
# Clean all variables that might be left by other script to avoid collusion
rm(list=ls(all=TRUE))
# load required libraries
library(ggplot2) # plotting lib
library(gridExtra) # arrange grids
library(dplyr)  # data manipuation
library(RColorBrewer) # color palettes
library(tm) # text mining
library(dbscan) # density-based clustering
library(wordcloud) # plots fequent terms
library(factoextra) # deals with cluster plotting. Provides cluster related utility methods 
library(cluster) # for gower similarity and pam
library(plot3D) # 3D plots
library(summarytools)
library(purrr)
library(wordcloud2)
library(rworldmap)
library(lubridate)
library(VIM)
source('utils.R') # supplementary code


# set summarytools global parameters
st_options(plain.ascii = F,       # This is very handy in all Rmd documents
      style = "rmarkdown",        # This too
      footnote = NA,             # Avoids footnotes which would clutter the result
      subtitle.emphasis = F,  # This is a setting to experiment with - according to
      dfSummary.graph.col = F
)  

# pick a palette
mainPalette = ggplotColours()
```

```{r global_options, include=FALSE}
# make the images flow nicely
knitr::opts_chunk$set(fig.pos = 'H', echo = T,comment = NA, prompt = F, 
                      cache = F, warning = F, message = F,  fig.align="center")
```


## Background

Without a deep understanding of how a company’s best current customers are segmented, a business often lacks the market focus needed to allocate and spend its precious human and capital resources efficiently. Furthermore, a lack of best current customer segment focus can cause diffused go-to-market and product development strategies that hamper a company’s ability to fully engage with its target segments. Together, all of those factors can ultimately impede a company’s growth.  
RFM (recency, frequency, monetary) analysis is a marketing technique used to determine quantitatively which customers are the best ones by examining how recently a customer has purchased (recency), how often they purchase (frequency), and how much the customer spends (monetary).

## Objective

The objective of customers segment according to their purchase history, is to turn them into loyal customers by recommending products of their choice.

# Apply the Ethical ML framework

## In the Problem definition and scope step

In the problem identification, we all agree in the gropup 9 that we want to improve customer loyalty by finding segments that help to search for strategies the comerce can apply for every segment.

Individuals are not impacted cause we don't have any PII in the dataset we are using and we will not use aggregate information from another sources.


We can see that There could be a risk to individualize each person in the dataset if someone can access to another dataset that relate the invoice or the customer ID with the PII

## In the Design step

To ensure we all interpreted the outputs of the model we clarify that we will find some cluster of customers according to its older buying behavior to give a tool to help design marketing strategies. 

## In the data collection and retention step

We just use data from the dataset proivided by kaggle made by UCI Machine Learning Repository.

The analysis about minorities we can do it based in the country feature of the dataset to see if some are underrepresented.

## In data processing step

We can identify a risk because we have two columns that could be used to re-identification, customerID and InvoiceNo, and it could be masked to de-identify the data.

## In the model prototyping and QA testing step




1 Data risk awareness

We commit to develop and improve reasonable processes and infrastructure to ensure data and model security are being taken into consideration during the development of machine learning systems.
We commit to prepare for security risks through explicit efforts, such as educating relevant personnel, establishing processes around data, and assess implications of ML backdoor.

2 Trust by privacy

We commit to build and communicate processes that protect and handle data with stakeholders that may interact with the system directly and/or indirectly.
One key way to establish trust with users and relevant stakeholders is by showing the right process and technologies are in place to protect personal data.
We should make explicit effort to understand the potential implications of metadata involved, and whether the metadata can expose unexpected personal information from relevant users or stakeholders.
Fortuantely in our dataset there are not sensitive data.There is no explicit personal information.The only feature related to personal information is CustomerID.We should not put any file related to the detailed person to the server.

3 Displacement strategy

We commit to identify and document relevant information so that business change processes can be developed to mitigate the impact towards workers being automated.
When planning the rollout of a new technology to automate a process, there are a number of people who's role or at least responsibilities will be automated. If this is not taken into consideration, these people will not have a transition plan and it won't be possible to fully benefit from the time and resources gained from the automation.

We should make sure they are able to raise the relevant concerns when business change or operational transformation plans are being set up, as this would make a significant positive impact in the rollout of the technology.

4 Bias evaluation

As developer it is important to obtain an understanding of how potential biases might arise. Once the different sub-categories for bias are identified it's possible to evaluate the results on a breakdown based on precision, recall and accuracy for each of the potential inference groups.
We checked through our system, there is no bias in our system.

# Data Analysis

Typically e-commerce datasets are proprietary and consequently hard to find among publicly available data. However, The UCI Machine Learning Repository has made this dataset containing actual transactions from 2010 and 2011.The data set used for this research contains 540k of transaction from UK retailer. The data has been sourced from [Kaggle](https://www.kaggle.com/carrie1/ecommerce-data). 


## Data Dictionary


Column Name                 | Column Description  
----------------------------| ------------------- 
InvoiceNo                   | Unique ID to identify each Invoice
StockCode                   | Unique ID for each item in stock
Description                 | A short description for each item
Quantity                    | Number of items bought
InvoiceDate                 | Invoice Date
UnitPrice                   | The price of each item
CustomerID                  | Unique ID for each custumer
Country                     | The country were the custumer lives


## Data Exploration

Firstly we are going to load and examine content and statistics of the data set

```{r}
data = read.csv("../data/data.csv", header = T, 
                na.strings = c("NA","","#NA"),sep=",")
```

```{r dataset_summary1, echo=FALSE, results="asis"}
print(dfSummary(data, valid.col = F, max.distinct.values = 3, heading = F),
      caption = "\\label{tab:dataset_summary1} Online Retail Dataset Summary", scalebbox = .9)
```

From the above summary, we can find that there are some negative values for Quantity and UnitPrice.These values don't make sense, so we'll delete them directly.

```{r}
customerData <- data %>% 
  mutate(Quantity = replace(Quantity, Quantity<=0, NA),
         UnitPrice = replace(UnitPrice, UnitPrice<=0, NA))


customerData = customerData %>%filter(complete.cases(.)) 
```
##Missing data
```{r plot_notes2,  echo=FALSE, fig.align="center", fig.cap="Missing data"}

a = aggr(data)
plot(a)

```

```{r}
summary(a)
```
There are some missing data for CustomerID and Desciption, we just remove them directly considering we have enough data.

```{r dataset_summary2, echo=FALSE, results="asis"}
print(dfSummary(customerData, valid.col = F, max.distinct.values = 3, heading = F),
      caption = "\\label{tab:dataset_summary2} Online Retail Dataset Summary", scalebbox = .9)
```


We need do some some data transformation and add one new variant total.
```{r}
customerData <- customerData %>% 
  mutate( InvoiceDate=as.Date(InvoiceDate, '%m/%d/%Y %H:%M'), 
          CustomerID=as.factor(CustomerID),
          Country = as.character(Country))

customerData <- customerData %>% 
  mutate(total = Quantity*UnitPrice)

glimpse(customerData)

```

```{r results='hold'}
# histogram with added parameters
hist(data$Quantity,
main="Quantity of purchase",
xlab="Quantity",
breaks=100000,
xlim=c(0,100),
col="darkmagenta",
freq=FALSE
)
```

```{r results='hold'}
# histogram with added parameters
hist(data$UnitPrice,
main="UnitPrice of purchase",
xlab="UnitPrice",
breaks=100000,
xlim=c(0,100),
col="darkmagenta",
freq=FALSE
)
```


```{r results='hold'}
# histogram with added parameters
hist(customerData$total,
main="total",
xlab="total",
breaks=100000,
xlim=c(0,500),
col="darkmagenta",
freq=FALSE
)
```

#### Descriptive Features.

*Description* is free-text features that might provide additional insights about the customer shopping. We are going to take a close look at this feature and decide if we could utilize it.

Lets' begin with the *Description*
```{r plot_notes,  echo=FALSE, fig.align="center", fig.cap="Most Common Words in Description"}
description = data %>%  mutate(text = trimws(Description)) %>% filter(!is.na(text)) %>% select(text)
pCorups = VCorpus(VectorSource(description))
pCorups  = tm_map(pCorups , removeNumbers)
pCorups  = tm_map(pCorups , removePunctuation)
pTermMatrix = tm::TermDocumentMatrix(pCorups)
tmp = as.matrix(pTermMatrix)
tmp = sort(rowSums(tmp),decreasing=T)
tmp = data.frame(word = names(tmp),freq=tmp)
set.seed(5673)
wordcloud(words = tmp$word, freq = tmp$freq, min.freq = 10,max.words=160, random.order=F,
          rot.per=0.35, colors=mainPalette)

#set.seed(5673)
#wordcloud2(data = tmp, size = 2)
```
From the word cloud, we can get some highly frequently used words such as set,red,box,lunch,blue,box,paper,glass.

Unfortunately *Description* feature does not provide more knowledge to what the others features already supply. Thus it will be dropped.

Country gives information about the country were the customer lives.

```{r}
length(unique(data$Country))
```
The custumers are from 38 different countries. Lets visualize this.


```{r plot_notes3,  echo=FALSE, fig.align="center", fig.cap="customer country distribution"}
#library(rworldmap)
countries <- as.data.frame(table(data$Country))
colnames(countries) <- c("country", "value")


matched <- joinCountryData2Map(countries, joinCode="NAME", nameJoinColumn="country")

mapCountryData(matched, nameColumnToPlot="value", mapTitle="Customer Country Distribution", catMethod = "pretty", colourPalette = "heat")


```

```{r plot_notes1,  echo=FALSE, fig.align="center", fig.cap="Countries Description"}
data %>%
  group_by(Country) %>%                              # calculate the counts
  summarize(counts = n()) %>%
  arrange(counts) %>%                                # sort by counts
  mutate(Country = factor(Country, Country)) %>%     # reset factor
  ggplot(aes(x=Country, y=counts)) +                 # plot 
    geom_bar(stat="identity") +                      # plot histogram
    coord_flip()                                     # flip the coordinates

```
```{r }

data1 <- data
data1$InvoiceDate <- mdy_hm(data$InvoiceDate) 


head(data1)

```
We now have the data transformed into datetime data. From the variable InvoiceDate we can extract the year, month, day and time.


```{r}
data1$InvoiceYear <- year(data1$InvoiceDate)
data1$InvoiceMonth <- month(data1$InvoiceDate)
data1$InvoiceWeekday <- wday(data1$InvoiceDate)
data1$InvoiceHour <- hour(data1$InvoiceDate)

```

Here we have the number of transactions per month for 2011.
```{r }
timedata <- data1 %>% 
  filter(InvoiceYear==2011) %>% 
  count(InvoiceMonth)  #count the number of invoices per month for 2011

ggplot(timedata, aes(InvoiceMonth, n)) +  #plot the number of invoices per day               
  geom_col(fill = "purple") +
  labs(x="Month", y="Number of invoices")
```

It seems that the number of transactions is rising from September and the highest in November. In december the lowest number of transactions is performed.

Lets explore which days are the most busy ones
```{r }
timedata <- data1 %>% 
  filter(InvoiceYear==2011) %>% 
  count(InvoiceWeekday)

ggplot(timedata, aes(InvoiceWeekday, n)) +  #plot the number of invoices per day               
    geom_col(fill = "purple") +
  labs(x="Week", y="Number of invoices") 
```

Most transactions are placed on monday, tuesday, wednesday and thursday.
```{r }
timedata <- data1 %>% 
 filter(InvoiceYear==2011) %>% 
  count(InvoiceHour)

ggplot(timedata, aes(InvoiceHour, n)) +  #plot the number of invoices per day               
    geom_col(fill = "purple") +
  labs(x="hour", y="Number of invoices") 
```
## Data Preparation

### Calculate RFM
To implement the RFM analysis, we need to take steps to get the rfm values:

1. Find the most recent date for each customer ID and calculate the days to the 2012-01-01, to get the recency data.
2. Calculate the quantity of transactions of a customer, to get the frequency data
3. Sum the amount of money a customer spent and divide it by frequency, to get the amount per transaction on average, that is the monetary data.

```{r results='hold'}

cd_RFM <- customerData %>% 
  group_by(CustomerID) %>% 
  summarise(recency=as.numeric(as.Date("2012-01-01")-max(InvoiceDate)),
            frequenci=n_distinct(InvoiceNo), monitery= sum(total)/n_distinct(InvoiceNo),
            country = max(Country)
      
            ) 

summary(cd_RFM)

head(cd_RFM)
```
```{r dataset_summary, echo=FALSE, results="asis"}
print(dfSummary(cd_RFM, valid.col = F, max.distinct.values = 3, heading = F),
      caption = "\\label{tab:dataset_summary} Online Retail Dataset Summary", scalebbox = .9)
```


```{r results='hold'}
# histogram with added parameters
hist(cd_RFM$recency,
main="recency of customer",
xlab="recency",
xlim=c(20,400),
col="darkmagenta",
freq=FALSE
)
```

```{r results='hold'}
# histogram with added parameters
hist(cd_RFM$frequenci,
main="frequenci of customer",
xlab="frequenci",
breaks=100,
xlim=c(0,50),
col="darkmagenta",
freq=FALSE
)
```


```{r results='hold'}
# histogram with added parameters
hist(cd_RFM$monitery,
main="monitery of customer",
xlab="monitery",
breaks=100,
xlim=c(0,10000),
col="darkmagenta",
freq=FALSE
)
```

Because the data is realy skewed, we use log scale to normalize
```{r}
cd_RFM$monitery <- log(cd_RFM$monitery)
hist(cd_RFM$monitery,
main="monitery of customer",
xlab="monitery",
breaks=100,
col="darkmagenta",
freq=FALSE
)
```


```{r results='hold'}
cd_RFM1 = cd_RFM%>% 
dplyr::select(-CustomerID,-country)

summary(cd_RFM1)
```

```{r results='hold'}
cd_RFM2 <- cd_RFM1 %>% 
mutate(recency = scale(recency),
       frequenci = scale(frequenci),
       monitery = scale(monitery)
  
)

summary(cd_RFM2)
```

# Modeling and Evalutation

In this section we will apply various clustering methods to cluster the customer based rfm. We will use Partitioning clustering and Hierarchical clustering approaches. 

Before we apply clustering models to the dataset we should assess clustering tendency. In order to do so we will employ **Hopkins** statistics.

## Hopkins Statistics

Hopkins statistic is used to assess the clustering tendency of a dataset by measuring the probability that a given dataset is generated by a uniform data distribution.(Ref: \cite{mining}).
Let's calculate Hopkins (**H**) statistics for cd_RFM2:

The **H** value close to one indicates very good clustering tendency. The **H** value around or greater than 0.5 denotes poor clustering tendency(Ref: \cite{factoextra}). 

```{r}
H =  get_clust_tendency(cd_RFM2,n = 100, graph = F, seed = 6709)
print(H[["hopkins_stat"]])
```
Perfect! H value is very close to 1. The dataset is clustrable.


## Partitioning Clustering Approach 

At first, we use Elbow method to get optimal number of clusters for k-means clustering:
```{r }

set.seed(123)
# Elbow method
fviz_nbclust(cd_RFM2, kmeans, method = "wss") +
    geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")


```

It seems that the optimal number of clusters is 4.

Let's use kmeans to cluster the dataset.

```{r results='hold'}
set.seed(123)
k2 <- kmeans(cd_RFM2, centers = 4, nstart = 25)
k2
fviz_cluster(k2, data = cd_RFM2)


```

```{r results='hold'}
group <- k2$cluster
cd_RFM3 <- cbind(cd_RFM, group)
#write.csv(cd_RFM3, "../shiny/www/mydata1.csv")

```

## Hierarchical clustering approache 


```{r results='hold'}
set.seed(123)
d <- dist(cd_RFM2)
c <- hclust(d, method = 'ward.D2')

plot(c)
```



```{r results='hold'}
members <- cutree(c,k = 4)

table(members)
members
```
According the characteristic of every group, we can give a desription for every group as below.
group 1:
Champions	
Bought recently, buy often and spend the most!	
Reward them. 
Can be early adopters for new products. Will promote your brand.

group 2:
Recent Customers	
Bought most recently, but not often.	
Provide on-boarding support, give them early success, start building relationship.

group 3:
Hibernating	
Last purchase was long back, low spenders and low number of orders.	Offer other relevant products and special discounts. 
Recreate brand value.

group 4:
Promising	
Recent shoppers, but haven’t spent much.	
Create brand awareness, offer free trials

## Clustering Method Evaluation 

We have applied two different clustering algorithm. Choosing between k-means and hierarchical clustering is not easy. We compare the two kinds of groups with the actual expected result, we decided to adopt k-means. 


# Model Deployment

Fortunately we can state that the clustering methods were  effective for the selected dataset. We do believe it might have a real live application. The model can segment customer successfully.

# Conclusion

We selected **e-commerce** dataset hoping to discover the relationship between various attributes, which would segment the customer into different groups. 

We spent significant efforts parsing and cleaning the data. Then we separated redundant and useful features. We add new features according to our requirement.

We also processed descriptive features applying data mining techniques. We counted the most frequently used terms to understand the content of the features. We counted the words. we successfully identified the most common words and phrase .

When the data preprocessing was done we measured Hopkins statistics to evaluate cluster tendency of the data set. The result was satisfactory; we proceeded with the clusterization. 

We have applied two different clustering algorithm. Choosing between k-means and hierarchical clustering is not easy. We compare the two kinds of groups with the actual expected result, we decided to adopt k-means. 

Overall we were able to apply unsupervised learning to reach our goal, and also we develop one shiny app to present our product.



\bibliography{RJreferences}


# Note from the Authors

This file was generated using [_The R Journal_ style article template](https://github.com/rstudio/rticles), additional information on how to prepare articles for submission is here - [Instructions for Authors](https://journal.r-project.org/share/author-guide.pdf). The article itself is an executable R Markdown file that could be [downloaded from Github](https://github.com/ivbsoftware/big-data-final-2/blob/master/docs/R_Journal/big-data-final-2/) with all the necessary artifacts.
